<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="bg-gray-50 text-gray-900">

<section id="stage-2" class="stage-b py-16 px-4 md:px-16">
    <div class="flex flex-row max-w-7xl mx-auto">
        <div class="w-5/12 pr-10">
            <h1 id="title-header" class="text-4xl font-bold mb-4">Discount Checker</h1>
            <p class="mb-6 text-lg leading-relaxed">
                Discount Checker is a project I originally created as my CS50x final project.
                That version was very simple, could only run locally and was more like a proof of concept.
                What it has become is a full stack, production grade project built from scratch.
            </p>
            
            <h1 class="text-3xl font-semibold mb-3">What does Discount Checker do?</h1>
            <p id="border-p" class="text-lg leading-relaxed">
                The goal of Discount Checker is to have all the products you want from different webshops in one place.
                You won't need to check 5 different websites to check if the product you want is on sale,
                you can simply check one website and see all your products in one place.
            </p>
        </div>

    <div class="flex-1 flex w-full justify-end">
        <img 
            src="static/images/website-2.png" 
            alt="Discount Checker Website" 
            class="w-full rounded-lg shadow"
        >
    </div>
        </div>

    <div class="max-w-7xl mx-auto mt-12 grid md:grid-cols-2 gap-8">
        <div>
            <h1 class="text-3xl font-semibold mb-3">How was it made?</h1>
            <p class="text-lg leading-relaxed">
                Discount Checker is a web application built in Python using tools such as Flask for the back-end, SQLAlchemy for the ORM, Celery/RedisMQ for rate limiting and task scheduling.
                It is deployed using Docker, nginx and waitress.
                It uses a custom built web scraping API built using FastAPI, Playwright for headless browser, a rotating residential proxy against bot detection and nginx and gunicorn for deployment.
            </p>
        </div>

        <div class="flex flex-col gap-6 pl-16">
            <div class="flex flex-col md:flex-row items-center gap-[5.1rem]">
                <h1 class="text-xl font-medium">Flask app</h1>
                <a target="_blank" href="https://github.com/NordinCoding/discountChecker">
                    <button class="bg-gray-800 hover:bg-gray-700 text-white px-4 py-2 rounded flex items-center">
                        <img src="static/images/github.png" class="w-6 h-6 mr-2" alt="GitHub">
                        View Repo
                    </button>
                </a>
            </div>
            <div class="flex flex-col md:flex-row items-center gap-4">
                <h1 class="text-xl font-medium">Web Scraper API</h1>
                <a target="_blank" href="https://github.com/NordinCoding/fastAPI/tree/main/fastAPI">
                    <button class="bg-gray-800 hover:bg-gray-700 text-white px-4 py-2 rounded flex items-center">
                        <img src="static/images/github.png" class="w-6 h-6 mr-2" alt="GitHub">
                        View Repo
                    </button>
                </a>
            </div>
        </div>
    </div>
</section>

<section id="stage-3" class="stage-c py-16 px-4 md:px-16 bg-gray-100">
    <div class="max-w-7xl mx-auto">
        <div id="stage-3-title" class="mb-8">
            <h1 class="text-4xl font-bold border-b-4 border-gray-800 inline-block pb-2">Code</h1>
        </div>

        <div id="stage-3-div" class="grid md:grid-cols-2 gap-12">
            <div class="code-segment space-y-4">
                <h1 class="text-3xl font-semibold">Standardising URLs</h1>
                <p id="standardise-text" class="text-lg leading-relaxed">
                    I very quickly figured out how complex rescraping a product database can be due to differences in user input but also due to inconsistencies in URLs.<br>
                    To combat this, I standardise all my URLs and remove their inconsistent trailing data before entering them into the database.
                </p>
                <div class="code-block-div-wrapper border rounded-lg p-4 bg-white shadow">
                    <div>
                        <img src="static/images/standardise_url.png" alt="Standardising URLs Code" class="w-full rounded-lg">
                    </div>
                    <div class="drawer-div hidden mt-4">
                        <div class="explanation-div space-y-3">
                            <p id="standardise-1">Clean up URL.</p>
                            <p id="standardise-2">
                                Bol.com URLs use a pattern of "/?" before trailing data.
                                If it exists, I use the index of "/?" to slice off the trailing data.
                            </p>
                            <p id="standardise-3">
                                I standardise all URLs so my deduplication logic stays consistent.
                                I do this by adding "https://www." if the URL doesn't already include it.
                            </p>
                        </div>
                        <div class="expand-div mt-2 flex items-center cursor-pointer">
                            <h3 class="font-medium mr-2">Explanation</h3>
                            <img src="static/images/arrow.png" class="w-4 h-4">
                        </div>
                    </div>
                </div>

                <h3 class="text-sm mt-2">
                    This snippet is from 
                    <a target="_blank" href="https://github.com/NordinCoding/discountChecker/blob/main/modules/functions.py#L69" class="text-blue-600 underline">
                        functions.py
                    </a> 
                    in line 69
                </h3>
            </div>

            <div id="deduplication-div" class="code-segment space-y-4">
                <h1 class="text-3xl font-semibold">Preventing duplicates in the Database</h1>
                <p class="text-lg leading-relaxed">
                    There is a possibility that multiple users will want the same product in their table. To account for this I added deduplication logic,<br> 
                    not only does this speed up the request time because it pulls data straight from the database, but it also saves resources by not having to request the API.
                </p>
                <h3 class="font-medium">The deduplication logic:</h3>
                <div class="border rounded-lg bg-white shadow p-4 overflow-x-auto w-[50rem]">
<pre class="text-sm font-mono bg-gray-100 p-4 rounded-lg overflow-x-auto">
Is the requested product already in the databases products table?
    <span class="font-bold text-blue-600">if it is:</span>
        Check if the product also exists in the userProducts table for the current userID
        
        <span class="font-bold text-blue-600">if it is:</span>
            Alert the user that the product is already in their table and don't add it
        <span class="font-bold text-blue-600">else:</span>
            Add the product to userProducts with the existing productID and userID
    <span class="font-bold text-blue-600">else:</span>
        Request the API and add the product data to the products and userProducts tables
</pre>
            </div>
                <h3 class="text-sm mt-2">
                    To see the code, look at the "/add_product" route in: 
                    <a target="_blank" href="https://github.com/NordinCoding/discountChecker/blob/main/app.py#L193" class="text-blue-600 underline">
                        app.py
                    </a> on line 193
                </h3>
            </div>
        </div>


<div class="code-segment-2 mt-16 space-y-6">
    <div class="title mb-4">
        <h4 class="text-3xl font-bold border-b-2 border-gray-800 inline-block pb-2 mb-5">Data Flow</h4>
    </div>

    <div id="data-flow-div" class="relative flex flex-col items-center space-y-4">
        <div class="flex flex-row gap-[20rem]">
            <div class="flex flex-col items-center space-y-4 z-10">
                <div class="data-box w-60 p-4 bg-white shadow rounded-lg text-center">User input</div>
                <div class="flow-arrow text-2xl">↓</div>
                <div class="data-box w-60 p-4 bg-white shadow rounded-lg text-center">Web Server Back-end Validation</div>
                <div class="flow-arrow text-2xl">↓</div>
                <div class="data-box w-60 p-4 bg-white shadow rounded-lg text-center">API Request</div>
                <div class="flow-arrow text-2xl">↓</div>
                <div class="data-box w-60 p-4 bg-white shadow rounded-lg text-center">Celery Task Queue</div>
                <div class="flow-arrow text-2xl">↓</div>
                <div class="data-box w-60 p-4 bg-white shadow rounded-lg text-center">API Response</div>
                <div class="top-[420px] md:top-[500px] flex space-x-6 md:space-x-8">
                    <div class="flex flex-col items-center space-y-4">
                        <div class="flow-arrow text-2xl">↓</div>
                        <div class="data-box w-[18rem] p-4 bg-white shadow rounded-lg text-center">
                            Database Storage
                        </div>
                    </div>
                <div class="flex flex-col items-center space-y-4">
                    <div class="flow-arrow text-2xl">↓</div>
                    <div class="data-box w-[18rem] p-4 bg-white shadow rounded-lg text-center">
                        Front-end Table Display
                    </div>
                </div>
                </div>
            </div>

            <div class="flex flex-col items-center space-y-4 z-10">
                <div class="data-box w-[18rem] p-4 bg-white shadow rounded-lg text-center">
                    Input in the form of a URL
                </div>
                <div class="flow-arrow text-2xl">↓</div>
                <div class="data-box w-[18rem] p-4 bg-white shadow rounded-lg text-center">
                    Back-end checks if the URL is from a supported website
                </div>
                <div class="flow-arrow text-2xl">↓</div>
                <div class="data-box w-[18rem] p-4 bg-white shadow rounded-lg text-center">
                    Web server calls the request_API() Celery task
                </div>
                <div class="flow-arrow text-2xl">↓</div>
                <div class="data-box w-[18rem] p-4 bg-white shadow rounded-lg text-center">
                    Request enters Queue with a concurrency of 1to not overload the API server
                </div>
                <div class="flow-arrow text-2xl">↓</div>
                <div class="data-box w-[18rem] p-4 bg-white shadow rounded-lg text-center">
                    Web Server receives the product data from the API in the form of a dictionary
                </div>
                <div class="top-[420px] md:top-[500px] flex space-x-6 md:space-x-8">
                    <div class="flex flex-col items-center space-y-4">
                        <div class="flow-arrow text-2xl pl-10">↓</div>
                        <div class="data-box w-[18rem] p-4 bg-white shadow rounded-lg text-center">
                            Product data gets stored in the databases products and userProducts tables
                        </div>
                    </div>
                <div class="flex flex-col items-center space-y-4">
                    <div class="flow-arrow text-2xl pr-10">↓</div>
                    <div class="data-box w-[18rem] p-4 bg-white shadow rounded-lg text-center">
                        Front-end gets the product data from the Back-end and displays it for the user
                    </div>
                </div>
                </div>
            </div>
        </div>
    </div>
</div>

                
    <div class="max-w-7xl mx-auto space-y-12">
        <div class="space-y-6 pt-12">
            <h1 class="text-3xl font-bold">Handling an empty product table</h1>
            <p class="text-lg">I really didn't like how the product table looked when it was empty, so I decided to add a message when the table is empty.</p>

            <div class="grid md:grid-cols-2 gap-8">
                <div class="space-y-4">
                    <img id="empty-table-img-1" src="static/images/empty_table_2.png" class="w-full rounded-lg shadow">
                    <h3 class="text-sm text-gray-600">
                        This snippet is from 
                        <a href="https://github.com/NordinCoding/discountChecker/blob/main/templates/index.html#L142" target="_blank" class="text-blue-600 underline">
                            index.html
                        </a> 
                        in line 142
                    </h3>
                    <div class="space-y-2 text-lg text-gray-700">
                        <p>This function checks for an empty table on page load. <br> checkRows is a simple function that checks the amount of rows currently in the users table.</p>
                        <p>If the amount of current rows is 0, <b>emptyTableMessage()</b> creates a new row and shows a simple message instructing the user on how to add a product.</p>
                        <p>When a product is removed, the same process happens: it checks the current row count and if 0, adds the empty table message.</p>
                    </div>
                </div>

                <div class="space-y-4">
                    <img id="empty-table-img-2" src="static/images/empty_table.png" class="w-full rounded-lg shadow">
                    <h3 class="text-sm text-gray-600">
                        This snippet is from 
                        <a href="https://github.com/NordinCoding/discountChecker/blob/main/static/js/main.js#L165" target="_blank" class="text-blue-600 underline">
                            main.js
                        </a> 
                        in line 165
                    </h3>
                </div>
            </div>
        </div>

        <div class="space-y-6">
            <h1 class="text-3xl font-bold">Handling real-time user input</h1>
            <p class="text-lg text-gray-700">
                Only sending valid URLs to my API is incredibly important, not just to prevent crashes but also to save unnecessary costs.
                I check if a supported domain is in the requested URL; if yes, it's sent to the API, otherwise the user gets a clear feedback message.
            </p>

            <div class="grid md:grid-cols-2 gap-8">
                <div class="space-y-4">
                    <div class="flex flex-col space-y-4">
                        <div>
                            <img src="static/images/feedback.png" class="w-full rounded-lg shadow">
                            <h3 class="text-sm text-gray-600">
                                This snippet is from 
                                <a href="https://github.com/NordinCoding/discountChecker/blob/main/templates/index.html#L98" target="_blank" class="text-blue-600 underline">
                                    index.html
                                </a> 
                                in line 98
                            </h3>
                        </div>
                        <div class="space-y-2 text-lg text-gray-700">
                            <p>I believe providing clear feedback to the user is vital for a good UX. I have a feedback message system that shows the status based on the situation.</p>
                            <p>For example, during a user request I disable the submit button, show a loading spinner with the text "Getting product data...". After the request finishes, they get feedback and the button is re-enabled.</p>
                        </div>
                    </div>
                </div>

                <div class="space-y-4">
                    <div>
                        <img src="static/images/fade_out.png" class="w-full rounded-lg shadow">
                        <h3 class="text-sm text-gray-600">
                            This snippet is from 
                            <a href="https://github.com/NordinCoding/discountChecker/blob/main/static/js/main.js#L24" target="_blank" class="text-blue-600 underline">
                                main.js
                            </a> 
                            in line 24
                        </h3>
                    </div>
                    <div class="space-y-2 text-lg text-gray-700">
                        <p>The <b>fadeOut()</b> function gets called after the feedback message is shown.</p>
                        <p>It takes the feedback div and submit button as arguments, disables the button, waits 3 seconds, sets opacity to 0, waits 1 more second, clears the message and re-enables the button.</p>
                        <p>
                            Once called, It immediately disables the button to prevent spam abuse, waits 3 seconds due to setTimeout,
                            set opacity to 0 to make it fade out, waits 1 more second, removes the contents of the messageDiv
                            and re-enable the button so the user can make their next request.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    <div class="max-w-7xl mx-auto space-y-12">

        <div class="space-y-6">
            <h1 class="text-4xl font-bold border-b-4 border-gray-800 inline-block pb-2">Web scraping</h1>
            <p class="text-lg text-gray-700">
                Initally for the CS50x version of Discount Checker I used BeautifulSoup4. 
                As good as BS4 is for basic scraping, I immediately ran into an issue when 
                upgrading this project. If the accept cookies pop up appeared, 
                it would block the page HTML with a JS script. 
                To fix this problem I did some research and found out headless browsers exist, 
                the ones I found were Playwright and Selenium, I went with playwright because 
                it's more modern, it's asynchronous and it has amazing documentation.
            </p>
        </div>

        <div class="space-y-8">
            <h1 class="text-3xl font-bold">Scraping Bol.com</h1>

            <div class="space-y-4">
                <h3 class="text-xl font-semibold">Bot detection</h3>
                <p class="text-gray-700 text-lg">
                    Bol.com has consistent and clean HTML, 
                    this made scraping the website in development super easy.
                    But when trying it out on a VPS it immediately stopped working 
                    due to bot detection, this led to a 3 week long battle against the bot detection
                    where I used different browser argument, different browsers,
                    random pauses to imitate user action and residential proxies.
                </p>
                <p class="text-gray-700 text-lg">
                    What ended up working was a combination, I found a community created upgrade 
                    to Playwright called Patchright which helped against bot detection, 
                    but that wasn't enough, what ended up making it consistent was a 
                    rotating residential proxy from Oxylabs. The combination of Patchright, 
                    browser arguments, random pauses and the proxies are what led me 
                    to a very conistent scraper.
                </p>
            </div>

            <div class="space-y-4">
                <h3 class="text-xl font-semibold">Getting the product data</h3>
                <p class="text-gray-700 text-lg">
                    When it comes to gathering the product data itsself, its very simple due to the consistent and clean HTML that Bol.com uses.<br>
                </p>

            <div class="flex flex-col md:flex-row gap-8 items-start">
                <div class="space-y-2">
                    <img 
                        src="static/images/bol_scraper1.png" 
                        class="md:max-w-[40vw] lg:max-w-[30vw] rounded-lg shadow"
                        alt="Scraper snippet"
                    >
                    <h3 class="text-sm text-gray-600">
                        This snippet is from 
                        <a href="https://github.com/NordinCoding/fastAPI/blob/main/fastAPI/mainPlay.py#L117" 
                        target="_blank" class="text-blue-600 underline">
                        mainPlay.py
                        </a> in line 117
                    </h3>
                </div>

                <div class="flex-1 space-y-2">
                    <img 
                        src="static/images/bolprice.png" 
                        class="w-full max-w-full md:max-w-[15vw] lg:max-w-[10vw] rounded-lg shadow"
                        alt="Bol.com pricing"
                    >
                    <h3 class="text-sm text-gray-600">Bol.com pricing</h3>
                </div>
            </div>
            <p class="text-gray-700 text-lg">
                For example, in this code snippet here, all it does is check if the element 
                of the original price exists(<b>"h-nowrap"</b>), 
                If this doesn't exist, this means the product is not on sale 
                and I set both prices to the standard price found.
                However if it does exist, this means the product is on sale and has 2 values, 
                the original price and the current price(<b>see image for example</b>).
                To find the original price I locate the element using the same class I used to check 
                for existence, get the inner HTML, clean up the data and return it.
            </p>
            <p class="text-gray-700 text-lg">
                The accept cookies and language selection pop up that Bol.com has doesn't always pop up due to my scraper storing session data.
                This helps the scraper be slightly faster due to not having to wait for the buttons to load and it won't have to click them.
                When the scraper returns an error however, I always delete the session data just to be safe.
            </p>
            </div>
        </div>

        <div class="space-y-8">
            <h1 class="text-3xl font-bold">Scraping Mediamarkt</h1>
            <p class="text-gray-700 text-lg">
                Scraping Mediamarkt couldn't have been more different. Instead of the bot detection being the challenge,
                the scraping of the product data was the challenge.
            </p>
            <p class="text-gray-700 text-lg">
                I used the same anti bot detection strategy that I used for Bol.com, which worked perfectly.
                But I quickly found out Mediamarkts HTML is very messy, not only was it very bloated,
                but it uses randomly generated class names that are re used often and change regularly
                which makes them impossible to use for consistent scraping.
            </p>
            <p class="text-gray-700 text-lg">
                After a few days of my scraper constantly breaking due to these class names,
                I started to look for a different approach which led me to find their "data-test" values.
                Their "data-test" values were used in the parent elements of the pricing elements, 
                never change and are not re used, so I use them instead of the classes for consistent scraping.
            </p>

            <div class="space-y-4">
                <h3 class="text-xl font-semibold">Extracting Original Price</h3>

                <div class="grid md:grid-cols-2 gap-8 items-start">
                    <div class="space-y-2">
                        <img src="static/images/mediamarkt_scraper1.png" class="w-full rounded-lg shadow">
                        <h3 class="text-sm text-gray-600">
                            This snippet is from 
                            <a href="https://github.com/NordinCoding/fastAPI/blob/main/fastAPI/mainPlay.py#L465" target="_blank" class="text-blue-600 underline">
                                mainPlay.py
                            </a> 
                            in line 465
                        </h3>
                    </div>

                    <div class="space-y-2 text-gray-700">
                        <p class="text-gray-700 text-lg">
                            This example showcases the way I extract the Original Price, as you can see its much different than with Bol.com.
                            the first part is the same, I check if the Original Price elememt exists, if not I set it to the Current Price.
                        </p>
                        <p class="text-gray-700 text-lg">
                            To actually get the content of the Original Price element, I first have to get all the content
                            that's stored in the parent element with the data-test value, I sift through this content with a simple loop to find
                            which element has the HTML that includes a <b>"€"</b>, once found I strip the whitespace and store it
                            in the ogPrice_html variable.
                        </p>
                        <p class="text-gray-700 text-lg">
                            Once the price is found, it has to be cleaned up before it gets sent back to my flask app, to do this I used regex.
                            The split method is used because the original p element holds the original price value twice, so i split it and take the last one
                            in the list because this is always the one that's correctly formatted.
                        </p>
                    </div>
                </div>

                <p class="text-gray-700 text-lg">
                    For example, a product with the original price of <b>"1099,–"</b>, will give me the content value of <b>"€1099.€1099.00"</b>.
                    which if split and taken the -1 index of, leaves the value <b>"1099.00"</b> which my Flask app expects.
                </p>
            </div>
        </div>
    </div>
</section>



<section id="stage-4" class="stage-d py-16 px-4 md:px-16 bg-gray-100">
    <div class="max-w-7xl mx-auto space-y-12">

        <div id="stage-4-title" class="mb-8">
            <h1 class="text-4xl font-bold border-b-4 border-gray-800 inline-block pb-2">System</h1>
        </div>

        <div class="md:flex md:space-x-12 space-y-12 md:space-y-0">

            <div class="md:w-1/2 space-y-8">

                <div class="space-y-4">
                    <h1 class="text-2xl font-semibold">Separation of concerns between Flask app and API</h1>
                    <p class="text-gray-700 text-lg">
                        My project uses 2 servers, both sersvers are shared VPS instances. The server running the Flask web app is very lightweight and only has 2 vCPU cores due to very low resource requirements. The other server is solely used for the API
                        and has 6 vCPU cores which is the minimum requirement needed to handle both user requests and scheduled requests simultaneously. 
                    </p>
                    <p class="text-gray-700 text-lg">
                        Keeping them separate avoids dependency bloat, simplifies resource management and keeps each environment easier to maintain. Keeping them separate also makes them easier to debug and they can't slow each other down.
                    </p>
                </div>

                <div class="space-y-4">
                    <h1 class="text-2xl font-semibold">Dual API Setup</h1>
                    <p class="text-gray-700 text-lg">
                        When I first built the webscraper API the only request type was user requests, this worked perfectly on a single API. When I implemented the scheduled rescrape and sent the scheduled requests to the same singular API 
                        while a user request was already being handled I immediately ran into a problem, I got the error message that there was 'no browser instance'. 
                    </p>
                    <p class="text-gray-700 text-lg">
                        After some research and testing I figured out that due to the API
                        already being in use, it could not create another browser instance. To solve this issue, I created an almost identical API to run parallel to the first API, I named these APIs user_scraper and scheduled_scraper respectively.
                        Due to this dual API setup, user requests and scheduled requests are handled on the same server, but on different APIs which means they will never interfere with each other.
                    </p>
                    <p class="text-green-700 text-lg">
                        Since I designed this setup, I have learned about browser pools in Playwright, however I still believe this Dual API setup is beneficial because it separates the concerns and allows for more control. If I were to implement browser pools
                        I can control how many browser instances each API can have.
                        <br><br>
                        For example, the scheduled rescrape doesn't need super high speeds because its a background job that users don't see, so with a high user base I could give the user_scraper API 5 browsers in a pool
                        and the scheduled_scraper API 3 browsers, this control wouldn't be as smooth and separated if all requests went to the same API.
                    </p>
                </div>

                <div class="space-y-4">
                    <h1 class="text-2xl font-semibold">User request handling vs Scheduled Request handling</h1>
                    <p class="text-gray-700 text-lg">
                        Both request types are handled differently. User requests are unpredictable and messy, To handle this, the input is cleaned and validated (as explained in the Code section), the user expects feedback so the Flask back end has to
                        receive errors and respond to them appropriately. Scheduled requests are very different, from the start the data it uses is already cleaned up and straight from the database so requests can be sent without worry, however my scheduled rescrape system
                        has retry logic.
                    </p>

                    <div class="md:flex flex-col space-y-4 md:space-y-0">
                        <div class="space-y-2">
                            <img src="static/images/retry.png" class="w-full rounded-lg shadow">
                            <h3 class="text-sm text-gray-600">
                                This snippet is from 
                                <a href="https://github.com/NordinCoding/discountChecker/blob/main/modules/functions.py#L145" target="_blank" class="text-blue-600 underline">
                                    functions.py
                                </a> 
                                in line 145
                            </h3>
                        </div>
                        <div class="space-y-2 pt-3">
                            <p class="text-green-700 text-lg">
                                This function gets called when the first rescrape fails, leading to 2 more attempts before returning the error message, logging it and skipping over the product. 
                            </p>
                            <p class="text-green-700 text-lg pb-5">
                                When I see this happen in the logs I always try out to scrape the product manually to see what has gone wrong, this has led me to finding many edge cases like detecting unavailable products.
                            </p>
                        </div>
                    </div>

                    <p class="bg-gray-100 p-4 rounded-lg overflow-x-auto font-mono text-sm">
                    2025-08-26 03:03:06 - INFO - User ID: None - Requesting rescrape of product: 32<br>
                    2025-08-26 03:03:22 - ERROR - User ID: None - Requested rescrape failed, retrying: {'error': 'Failed to scrape product data/alter product data', 'details': 'Page.wait_for_selector: Timeout 10000ms exceeded.\nCall log:\n  - waiting for locator(".bPkjPs") to be visible\n'}<br><br>
                    2025-08-26 03:03:22 - INFO - User ID: None - 1st retry on product: 32<br>
                    2025-08-26 03:03:22 - INFO - User ID: None - Requesting rescrape of product: 32<br>
                    2025-08-26 03:03:39 - INFO - User ID: None - Requested product successfully rescraped: {'name': 'MSI MAG 271QPX QD-OLED E2 - 27 inch - 2560 x 1440 (Quad HD) - 0.03 ms - 240 Hz', 'currentPrice': '559.00', 'ogPrice': '699.00'}<br>
                </p>

                <p class="text-green-700 text-lg">
                    In this snippet taken straight out of my logs, the scrape failed waiting for class '<b>.bPkjPs</b>' to be visible, I use this class to detect if the page has loaded because the class is used 10+ times on any Mediamarkt product page. 
                    Due to this I can assume the page took too long to load so my scraper returned a timeout, and on the first retry it successfully returned the product data, 
                    definitively meaning it was a connectivity error and not a code error.
                </p>
            </div>
        </div>

        <div class="md:w-1/2 space-y-8">

            <div class="space-y-4">
                <h1 class="text-2xl font-semibold">API in action</h1>
                <video class="w-full rounded-lg shadow" controls>
                    <source src="static/videos/retry.mp4">
                </video>
                <p class="text-gray-700 text-lg">
                    This video showcases the API in action with a manually triggered rescrape.
                    The window on the left contains a "Test" button which triggers the manual rescrape, this causes the page to freeze, however
                    in deployment this won't happen due to it being a scheduled task that runs in the background. 
                </p>
                <p class="text-gray-700 text-lg">
                    the top right window is the htop(system monitor) of my webscraper APIs server
                    which shows the API being in use, and in the bottom right window you can see the logs of the rescrape in action.
                </p>
            </div>

            <div class="space-y-4">
                <h1 class="text-2xl font-semibold">Rescraping userProducts daily and all other products weekly to save proxy costs</h1>
                <p class="text-gray-700 text-lg">
                    When and what to rescrape was a decision that I had to think about a lot. I really want fresh live data, but I also don't want to spend my entire paycheck on a portfolio project, So finding a balance was very important.
                    I eventually found that balance between keeping costs down and still having semi fresh data. 
                </p>
                <p class="text-gray-700 text-lg">
                    Because webshops don't update their prices every 30 minutes, checking once a day is enough and the only data that needs to be up to date is the data
                    the user is seeing, so my solution was to keep the user facing data fresh on daily basis, and all other data semi fresh on a weekly basis.
                </p>
                <p class="text-green-700 text-lg">
                    My database has 2 product tables, Products and userProducts, as the names imply the Products table is an overall table and holds every product ever requested, the userProducts table connects the userID(from the Users table)
                    and the productID(from the Products table). 
                    <br><br>
                    The Products table is rescraped weekly on Tuesday at 03:00 CEST to maintain semi fresh data in the entire database 
                    and because in the future I want to do some data analysis stuff like keeping record of the lowest price seen or creating price graphs for users.
                    <br><br>
                    The userProducts table is rescraped daily at 03:00 CEST besides on Tuesdays because that's when the weekly rescrape happens. This design leads to 
                    very fresh feeling user facing data and keeps the entire database semi fresh while keeping costs at a minimum.
                </p>
            </div>

            <div class="space-y-4">
                <h1 class="text-2xl font-semibold">Database Structure</h1>
                <div class="md:flex md:space-x-6 space-y-4 md:space-y-0 items-start">
                    <div class="flex flex-row space-y-2">
                        <div class="md:w-[40rem]">
                            <img src="static/images/db.png" class="w-full rounded-lg shadow">
                        </div>
                        <div class="flex flex-col w-1/2 gap-5 pl-3">
                            <div>
                                <p class="text-green-700 text-lg">
                                    The database is made up of 3 tables: <code>users</code>, <code>products</code> and <code>userProducts</code>. <code>userProducts</code> is a junction table that links users to the products they want to track using <code>userID</code> 
                                    and <code>productID</code>. 
                                </p>
                            </div>
                            <div>
                                <p class="text-green-700 text-lg">
                                    This design keeps the database simple while also allowing for multiple users to track the same product, which saves database space and API usage.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

        </div>
    </div>
</div>
</section>
    </div>
</body>
</html>
